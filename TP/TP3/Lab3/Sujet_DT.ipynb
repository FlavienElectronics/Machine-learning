{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1644e8",
   "metadata": {},
   "source": [
    "**Group :**\n",
    "\n",
    "**Name, surname 1**\n",
    "\n",
    "**Name, surname 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496cb13",
   "metadata": {},
   "source": [
    "## Advice\n",
    "### Work\n",
    "- **Read the whole subject before starting**\n",
    "- Search for the documentation (Google) before asking a teacher\n",
    "- The work will be evaluated based on this notebook:\n",
    "    - Answer questions in the notebook.\n",
    "    - Insert your code here and execute it so that the output stays displayed for the teacher.\n",
    "\n",
    "### ChatGPT\n",
    "- You can use ChatGPT ONLY to answer specific questions, get introductory explanations on machine learning libraries, get example codes. Be aware that there is no guaranty in the answer of ChatGPT (even with the paying licence).\n",
    "- Do not use ChatGPT to work in your stead ! The goal is for you to learn the manipulation of machine learning basic methods yourself. Teachers are used to look at student's works and ChatGPT is not good to fake it.\n",
    "=> In case of a doubt of ChatGPT use (or plagiarism between groups), the students will be summoned and evaluated on an oral presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1737fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "475a964d",
   "metadata": {},
   "source": [
    "# Lab: Classification with Decision Trees\n",
    "\n",
    "**Objectives of the practical work:**\n",
    "\n",
    "1. Learn how to build decision trees with scikit-learn Â \n",
    "2. Be familiar with some parameters and visualization tools\n",
    "3. Use a real-case dataset (COMPASS ) as an example\n",
    "4. Evaluate diverse trees in terms of training and testing accuracies with different parameters\n",
    "5. Study the impact of some parameters on the sensitivity aspect\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3db85d",
   "metadata": {},
   "source": [
    "## PART 1: Basic steps \n",
    "\n",
    "The following are basic instructions to start with decision trees. You need to execute them one by one to understand the basic steps for learning decision trees. Once you get familiar with the different steps, you will be working on the compass dataset. \n",
    "\n",
    "The decision tree package that we use is from scikit-learn. The full documentation of decision trees are available at https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "Take a moment to briefly consult the documentation.\n",
    "\n",
    "We need first to include some libraries: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11530ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt # for a good visualization of the trees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ce240f",
   "metadata": {},
   "source": [
    "The following is a basic example for binary classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the training set \n",
    "# Each example in X has 4 binary features\n",
    "X = [[0, 0, 1, 0], [0, 1, 0, 1] , [1, 1, 0, 0] , [1, 0, 1, 1] , [0, 0, 0, 1] , [1, 1, 1, 0]]\n",
    "\n",
    "# Y is the classes associated with the training set. \n",
    "# For instance the label of the first and second example is 1; of the third example is 0, etc\n",
    "Y = [1, 1, 0, 0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f396bd",
   "metadata": {},
   "source": [
    "We construct a decision tree using the default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75d5a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe42ed",
   "metadata": {},
   "source": [
    "Now we can ask the decision tree to predict the outcome for unknown examples. \n",
    "For instance we can ask a prediction for the three examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de16e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([[1,1,1,1] , [0,1,0,0] , [1,1,0,1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466faea1",
   "metadata": {},
   "source": [
    "The result is an array of the 3 predicted labels (one for each example): `array([0, 1, 0])`\n",
    "\n",
    "## PART 2 : Visualization \n",
    "\n",
    "There are many ways to visualize a decision tree. The first one is very basic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67433f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = tree.export_text(clf)\n",
    "print(text_representation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65508fca",
   "metadata": {},
   "source": [
    "We can use a more readable and visual way as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94629400",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,7))\n",
    "_ = tree.plot_tree(clf, \n",
    "        feature_names= (\"f1\",\"f2\" , \"f3\", \"f4\"),\n",
    "        class_names= (\"false (0)\", \"true (1)\" ), \n",
    "        filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d0a912",
   "metadata": {},
   "source": [
    "Where:\n",
    "- `figsize` restrains the size of the plot,\n",
    "- `feature_names` gives the names of the different features,\n",
    "- `class_names` corresponds to human readable labels for each class,\n",
    "- `filled` is a boolean indicating a preference to show a colorful tree. \n",
    "\n",
    "\n",
    "**Tasks:**\n",
    "- Construct manually a new binary dataset (larger than the one above), associate some labels then study the tree built by default (similar to above). Give some fancy names to the binary features and classes for a visual interpretation. Be quick, the constructed dataset does NOT need to make sense.\n",
    "\n",
    "\n",
    "\n",
    "## PART 3: The compass dataset\n",
    "\n",
    "We study here the COMPASS dataset as a case study. Recall that it has been used in a legislative context for predicting recidivism in the U.S. That is, the tendency of a convicted criminal to re-offend\n",
    "\n",
    "\n",
    "Have a look at the original non-binary dataset ([https://www.kaggle.com/danofer/compass](https://www.kaggle.com/danofer/compass)) to understand the different features. Consider in particular the data used for fairness: propublicaCompassRecividism_data_fairml.csv\n",
    "\n",
    "\n",
    "**Understanding the dataset (from Kaggle) :**\n",
    "\n",
    "Take a moment to think about the following questions \n",
    "\n",
    "- What are the features? \n",
    "- How many examples in the dataset?\n",
    "- What are your expectations regarding the most important features? \n",
    "- Propose (informally) a way to reduce the dataset\n",
    "- There many ways to binarize the dataset. How do you propose to do so?\n",
    "\n",
    "\n",
    "\n",
    "**Analysing the dataset with machine learning (custom binarized dataset, NOT from Kaggle) :**\n",
    "\n",
    "Below, we use a binarized version of the dataset that is used in the FairCORELS library (https://github.com/ferryjul/fairCORELS) as well some of its tools. \n",
    "\n",
    "You need first to download the dataset and the tools file and put them in your work directory:\n",
    "\n",
    "- The dataset [compass.csv](compass.csv)\n",
    "- The set of tools [utils.py](utils.py)\n",
    "\n",
    "Load the binary dataset `compass.csv` as follows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from utils import load_from_csv\n",
    "\n",
    "train_examples, train_labels, features, prediction = load_from_csv(\"./compass.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb13fc2",
   "metadata": {},
   "source": [
    "Inspect each of these 4 objects.\n",
    "What do they represent? How many features? examples?\n",
    "We want to predict recidivism after 2 years, where is this information stored ?\n",
    "\n",
    "Have a look at the different parameters of the `DecisionTreeClassifier` class constructor. We will be studying three parameters: \n",
    "- splitter\n",
    "- max_depth \n",
    "- min_samples_leaf \n",
    "\n",
    "What do they represent? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c0eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e25f3e60",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73bc3bee",
   "metadata": {},
   "source": [
    "## EVALUATED PART:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed890317",
   "metadata": {},
   "source": [
    "In the following, we consider the  (binarized) Compas dataset that we studied in the Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4664e",
   "metadata": {},
   "source": [
    "Q1: A decision tree configuration is a set of parameters that one can use to build decision trees. Propose 6 configurations that are likely to provide different topologies and caracteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6debc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bc40ac6",
   "metadata": {},
   "source": [
    "Q2: Train a decision tree for each of the previous configurations on the full dataset. Run a solid evaluation on different trees (with different parameters) by randomly splitting the data 80% for training and 20% for test *multiple times*. Vizualize the resulting decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd14afa5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "602fdb5a",
   "metadata": {},
   "source": [
    "Q3: Propose an evaluation in terms of training and testing accuracies using $5$-cross validation on two decision trees that have different topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec5ba0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12b5ebe3",
   "metadata": {},
   "source": [
    "Q4: Propose an experimental study that shows the transition phase from underfitting to overfitting. Evaluate the impact (in terms of accuracy) of the three parameters : maximum depth, splitting criterion, and the minimum sample leafs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e4413",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b25669bd",
   "metadata": {},
   "source": [
    "Q5: Construct the confusion matrix on a particular good configuration (after explaining your choice). What are the most important parameters in terms of True/False Positive/Negative Rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1aaa7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c613349",
   "metadata": {},
   "source": [
    "Q6: Provide an evaluation of the fairness of the model based on the False Positive Rate. The \"utils.py\" file provides useful functions for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45b2d9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
